
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Appendix &#8212; Predicting Shopper Intentions</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8e5499552300ddf5d7adccae7cc3b70.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Model Exploration" href="model_exploration.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo-resized.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Predicting Shopper Intentions</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Predicting Shopper Intentions
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="data_exploration.html">
   Data Exploration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_preparation.html">
   Data Preparation &amp; Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="model_exploration.html">
   Model Exploration
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/appendix.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/appendix.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fappendix.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/appendix.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#violin-plots">
   1. Violin Plots
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example">
     1.1. Example
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#building-blocks">
     1.2. Building Blocks
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#box-plots">
       1.2.1. Box Plots
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id1">
         1.2.1.1. example
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id2">
         1.2.1.2. Building blocks
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#usage-and-interpretation">
         1.2.1.3. Usage and Interpretation
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kde-plots">
       1.2.2. KDE Plots
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id3">
         1.2.2.1. Example
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id4">
         1.2.2.2. Building blocks
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id5">
         1.2.2.3. Usage and Interpretation
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     1.3. Usage and Interpretation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pearson-s-chi-2-test-for-statistical-independence">
   2. Pearson’s
   <span class="math notranslate nohighlight">
    \(\chi^2\)
   </span>
   test for Statistical Independence
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#oversampling">
   3. Oversampling
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#smote-for-continuous-values">
     3.1.
     <strong>
      SMOTE
     </strong>
     for continuous values
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#smote-for-categorical-values">
     3.2.
     <strong>
      SMOTE
     </strong>
     for categorical values
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-theory">
   4. Learning Theory
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#statistical-learning-framework">
     4.1 Statistical Learning Framework
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-empirical-risk">
     4.2 The Empirical Risk
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#overfitting">
     4.3 Overfitting
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recrifying-overfitting">
     4.4. Recrifying Overfitting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#finite-hypothesis-classes">
       4.4.1 Finite Hypothesis Classes
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#realizability-assumption">
       4.4.2 Realizability Assumption
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#i-i-d-assumption">
       4.4.3 I.I.D. Assumption
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#putting-it-all-together">
       4.4.4 Putting it all together
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pac-learning">
     4.5. PAC Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalizing-the-model-removing-the-realizability-assumption">
     4.6 Generalizing the model - Removing the Realizability Assumption
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generalizing-the-model-extending-further-than-binary-classification">
     4.7 Generalizing the model - Extending further than Binary Classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#uniform-convergence">
     4.8 Uniform Convergence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recap">
     4.8 Recap
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vc-dimension">
     4.8 VC-dimension
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#k-fold-cross-validation">
   5. K-Fold Cross Validation
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">¶</a></h1>
<p>In this page we can find further development of concepts used in previous chapters.</p>
<div class="section" id="violin-plots">
<span id="appendix-violinplot"></span><h2>1. Violin Plots<a class="headerlink" href="#violin-plots" title="Permalink to this headline">¶</a></h2>
<p>Violin plots are a powerful tool that puts together the information from <em>Box plots</em> and <em>KDE Plots</em>.</p>
<div class="section" id="example">
<h3>1.1. Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_1_0.png" src="_images/appendix_1_0.png" />
</div>
</div>
<p>As we already said, a Violin plot is the combination of box plots and kde plots. Let’s have a look at them first.</p>
</div>
<div class="section" id="building-blocks">
<h3>1.2. Building Blocks<a class="headerlink" href="#building-blocks" title="Permalink to this headline">¶</a></h3>
<div class="section" id="box-plots">
<h4>1.2.1. Box Plots<a class="headerlink" href="#box-plots" title="Permalink to this headline">¶</a></h4>
<p>Box plots are used to depict numerical data through their quartiles.</p>
<div class="section" id="id1">
<h5>1.2.1.1. example<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">whis</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>

<span class="c1"># Median and quartiles</span>
<span class="n">med</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">//</span><span class="mi">2</span><span class="p">]</span>
<span class="n">fq</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span><span class="p">]</span>
<span class="n">tq</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">//</span><span class="mi">4</span><span class="o">*</span><span class="mi">3</span><span class="p">]</span>

<span class="c1"># Inter-quaritles range</span>
<span class="n">iqr</span> <span class="o">=</span> <span class="n">tq</span> <span class="o">-</span> <span class="n">fq</span>

<span class="c1"># Minimum and maximum</span>
<span class="n">mmin</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">tq</span> <span class="o">+</span> <span class="n">iqr</span><span class="o">*</span><span class="mf">1.5</span><span class="p">))]</span>
<span class="n">mmax</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">fq</span> <span class="o">-</span> <span class="n">iqr</span><span class="o">*</span><span class="mf">1.5</span><span class="p">))]</span>

<span class="c1"># Lines with text</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">fq</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;First quartile&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">fq</span> <span class="o">-</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s2">&quot;First quartile&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">tq</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Third quartile&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">tq</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s2">&quot;Third quartile&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">med</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Median&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">mmin</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Minimum&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">mmin</span> <span class="o">-</span> <span class="mf">0.3</span><span class="p">,</span> <span class="s2">&quot;Minimum&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">mmax</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Maximum&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.45</span><span class="p">,</span> <span class="n">mmax</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s2">&quot;Maximum&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_3_0.png" src="_images/appendix_3_0.png" />
</div>
</div>
</div>
<div class="section" id="id2">
<h5>1.2.1.2. Building blocks<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p>The main concept behind this representation is the <em>Percentile</em>:</p>
<p><em>A percentile is a score below which falls a given percentage of the data</em></p>
<p><span class="math notranslate nohighlight">\(
  P_n = \frac{n}{100}N
\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> is the desired percentile</p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the total number of observation</p></li>
</ul>
<p>The components needed to build this kind of plot are:</p>
<ol class="simple">
<li><p><strong>First Quartile (<span class="math notranslate nohighlight">\(P_{25}\)</span>)</strong>: <em>Median</em> of the <em>lower half</em> of the dataset.</p></li>
<li><p><strong>Second Quartile (<span class="math notranslate nohighlight">\(P_{75}\)</span>)</strong>: <em>Median</em> of the <em>upper half</em> of the dataset.</p></li>
<li><p><strong>Inter-Quartile Range (<span class="math notranslate nohighlight">\(P_{75}\)</span> - <span class="math notranslate nohighlight">\(P_{25}\)</span>)</strong>: Distance <em>between</em> the <em>thrid</em> and the <em>first</em> quartile</p></li>
<li><p><strong>Minimum (<span class="math notranslate nohighlight">\(P_{25} - 1.5\, \text{IQR}\)</span>)</strong>: 1.5 times the <em>IQR below the first</em> quartile</p></li>
<li><p><strong>Maximum (<span class="math notranslate nohighlight">\(P_{75} + 1.5\, \text{IQR}\)</span>)</strong>: 1.5 times the <em>IQR above the thrid</em> quartile</p></li>
<li><p><strong>Outliers</strong>: Any point falling <em>above the maximum</em> and <em>below the minimum</em></p></li>
</ol>
</div>
<div class="section" id="usage-and-interpretation">
<h5>1.2.1.3. Usage and Interpretation<a class="headerlink" href="#usage-and-interpretation" title="Permalink to this headline">¶</a></h5>
<p>The best feature of box plots is that they sum the data distribution with only 5 values (First/Second/Thrid Quartile, Maximum and Minimum). They give a clear, although rough understanding of the distribution of our data.</p>
</div>
</div>
<div class="section" id="kde-plots">
<h4>1.2.2. KDE Plots<a class="headerlink" href="#kde-plots" title="Permalink to this headline">¶</a></h4>
<p>Kernel Density Estimation Plots represent an <strong>estimated probability density function</strong> based on a finite data sample.</p>
<div class="section" id="id3">
<h5>1.2.2.1. Example<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span>
<span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">()</span>

<span class="c1"># generate sample data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">underlying_distrib</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">underlying_distrib</span><span class="p">[:</span><span class="mi">150</span><span class="p">],</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">data</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">underlying_distrib</span><span class="p">[</span><span class="o">-</span><span class="mi">150</span><span class="p">:],</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">samples</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># bandwidth parameter (standard deviation of each kernel)</span>
<span class="n">bandwidth</span><span class="o">=</span><span class="mi">1</span>

<span class="c1"># grid points</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">bandwidth</span>
<span class="n">xmax</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">bandwidth</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>

<span class="c1"># compute KDE by hand</span>
<span class="n">kde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">kdes</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
    <span class="n">kdes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="n">val</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">bandwidth</span><span class="p">))</span>
<span class="n">kde</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">kdes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># normalize</span>
<span class="n">norm_fact</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">integrate</span><span class="o">.</span><span class="n">simps</span><span class="p">(</span><span class="n">kde</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">kde</span> <span class="o">/=</span> <span class="n">norm_fact</span>


<span class="c1"># plot computed kde</span>

<span class="c1"># in scipy, the bandwidth is scaled by the standard deviation.</span>
<span class="c1"># In order to be able to compare our implementation with scipy&#39;s, we must</span>
<span class="c1"># divide the bandwidth by the standard deviation of the sample.</span>
<span class="n">bandwidth</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;seaborn&quot;</span><span class="p">,</span> <span class="n">bw_method</span><span class="o">=</span><span class="n">bandwidth</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">kde</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;KDE&quot;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>

<span class="c1"># plot single kernels</span>
<span class="k">for</span> <span class="n">kd</span> <span class="ow">in</span> <span class="n">kdes</span><span class="p">:</span>
    <span class="c1"># https://gsalvatovallverdu.gitlab.io/python/kernel_density_estimation/</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">kd</span><span class="o">/</span><span class="n">norm_fact</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Paired&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">ax</span><span class="o">.</span><span class="n">lines</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_linestyle</span><span class="p">(</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>

<span class="c1"># plot sample data</span>
<span class="n">sns</span><span class="o">.</span><span class="n">rugplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;Paired&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_5_0.png" src="_images/appendix_5_0.png" />
</div>
</div>
</div>
<div class="section" id="id4">
<h5>1.2.2.2. Building blocks<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<p>Kernel Density Estimate is a procedure used to estimate a density function, starting from a sample, originated from an unknown underlying distribution.
In order to build such estimated density function, we must chose an appropriate <strong>kernel</strong>.
We can choose among many different kernels, in our explanation we will use the normal kernel.
The following equation shows how we build the KDE:</p>
<div class="math notranslate nohighlight">
\[ {\displaystyle {\widehat {f}}_{h}(x)={\frac {1}{m}}\sum _{i=1}^{n}K{\Big (}{\frac {x-x_{i}}{h}}{\Big )},} \]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(K\)</span> is the Gaussian kernel centered in a given point <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> is the <em>bandwidth</em> of the kernel (the standard deviation). This parameter influences the smoothness of the estimated function.</p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> is a scaling factor. Seaborn’s implementation of KDE uses the integral of the sum of the kernels as the scaling factor.</p></li>
</ul>
<p>From an intuitive point of view, we are centering a Gaussian distribution in each of our sample points, and calling the sum of such distributions our KDE.</p>
</div>
<div class="section" id="id5">
<h5>1.2.2.3. Usage and Interpretation<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h5>
<p>KDE plots give a powerful representation of what we expect to be the underlying distribution of our sample data.
It is a non-parametric method, meaning that we do not need strong assumptions on the unknown distribution, but it is very sensible on the choice of bandwidth, and the amount of data.</p>
</div>
</div>
</div>
<div class="section" id="id6">
<h3>1.3. Usage and Interpretation<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>The Violin plot puts together the information of the Box and the KDE, making it more even more powerful information-wise.
The side effect is that it requires a deeper understanding in order to read it correctly, and so it can be intimidating, making it less common.</p>
<p>All these plots are commonly used to show differences among populations.</p>
</div>
</div>
<div class="section" id="pearson-s-chi-2-test-for-statistical-independence">
<span id="appendix-chi-squared"></span><h2>2. Pearson’s <span class="math notranslate nohighlight">\(\chi^2\)</span> test for Statistical Independence<a class="headerlink" href="#pearson-s-chi-2-test-for-statistical-independence" title="Permalink to this headline">¶</a></h2>
<p>The chi-squared test is a statistical <em>hypothesis test</em> carried out when the <em>test statistic</em> is chi-squared distributed under the <em>null hypothesis</em>.
The way we use Pearson’s chi-squared test to test for statistical independence is the following:</p>
<p>Suppose to have data from 1000 athletes concerning the brand of their shoes, and the final place. Both features are categorical, and suppose they are encoded ordinally.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">df_fake</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;shoes&quot;</span><span class="p">,</span> <span class="s2">&quot;podium&quot;</span><span class="p">])</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># There are 3 brands of shoes</span>
<span class="n">shoes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="c1"># There are 4 possible final positions: 1st, 2nd, 3rd and 0 for not-on-the-podium</span>
<span class="n">podium</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Let&#39;s insert some dependence between brand of shoes and final place</span>
<span class="n">podium_rigged_good</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">podium_rigged_bad</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">samples</span><span class="p">):</span>
    <span class="n">shoe</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">shoes</span><span class="p">)</span>
    <span class="c1"># brand 1 will be better than all the other</span>
    <span class="k">if</span> <span class="n">shoe</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">podium_rigged_good</span><span class="p">)</span>
    <span class="c1"># brand 2 will be worse</span>
    <span class="k">elif</span> <span class="n">shoe</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">podium_rigged_bad</span><span class="p">)</span>
    <span class="c1"># and brand 3 will give equal chance to all positions</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">podium</span><span class="p">)</span>

    <span class="n">df_fake</span> <span class="o">=</span> <span class="n">df_fake</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;shoes&#39;</span><span class="p">:</span> <span class="n">shoe</span><span class="p">,</span> <span class="s1">&#39;podium&#39;</span><span class="p">:</span> <span class="n">pos</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">df_fake</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>shoes</th>
      <th>podium</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>995</th>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>996</th>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>997</th>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>998</th>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>999</th>
      <td>3</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>1000 rows × 2 columns</p>
</div></div></div>
</div>
<p>Our <em>observation</em> is made up of two values (one for <em>shoes</em> and one for <em>podium</em>), and we want our <em><span class="math notranslate nohighlight">\(H_0\)</span></em> to say that <strong>the two random variables are statistically independent</strong>.
To carry on the test we must compute the <em>contingency table</em>, counting each occurrence for each pair of values.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_crosstab</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">df_fake</span><span class="p">[</span><span class="s1">&#39;shoes&#39;</span><span class="p">],</span> <span class="n">df_fake</span><span class="p">[</span><span class="s1">&#39;podium&#39;</span><span class="p">])</span>
<span class="n">data_crosstab</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>podium</th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
    </tr>
    <tr>
      <th>shoes</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>89</td>
      <td>124</td>
      <td>71</td>
      <td>75</td>
    </tr>
    <tr>
      <th>2</th>
      <td>98</td>
      <td>69</td>
      <td>70</td>
      <td>89</td>
    </tr>
    <tr>
      <th>3</th>
      <td>74</td>
      <td>75</td>
      <td>78</td>
      <td>88</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>In a standard chi-squared test, we would need to formulate our <em>null hypothesis</em> by choosing some values for the desired occurrences of each variables, and this would give us theoretical values for the expected values of our observation. The resulting test statistic would assume the form:</p>
<div class="math notranslate nohighlight">
\[{\displaystyle \chi ^{2}=\sum _{i=1}^{n}{\frac {(O_{i}-E_{i})^{2}}{E_{i}}}=N\sum _{i=1}^{n}{\frac {\left((O_{i}/N)-p_{i}\right)^{2}}{p_{i}}}}\]</div>
<p>Since in this case, our <span class="math notranslate nohighlight">\(H_0\)</span> is that the two variables are independent, we define our expected values as</p>
<div class="math notranslate nohighlight">
\[E_{{i,j}}=Np_{{i\cdot }}p_{{\cdot j}}\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\({\displaystyle p_{{i\cdot }}={\frac {O_{{i\cdot }}}{N}}=\sum _{{j=1}}^{c}{\frac {O_{{i,j}}}{N}}}\)</span> is the fraction of observations ignoring the column attribute</p></li>
<li><p><span class="math notranslate nohighlight">\({\displaystyle p_{{\cdot j}}={\frac {O_{{\cdot j}}}{N}}=\sum _{{i=1}}^{r}{\frac {O_{{i,j}}}{N}}}\)</span> is the fraction of observations ignoring the row attribute</p></li>
</ul>
<p>Now we</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2_contingency</span>

<span class="n">chi_stat</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dof</span><span class="p">,</span> <span class="n">expected</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">data_crosstab</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;chi-squared statistic:</span><span class="se">\t</span><span class="si">{</span><span class="n">chi_stat</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;p-value:</span><span class="se">\t\t</span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>chi-squared statistic:	22.0507
p-value:		0.0012
</pre></div>
</div>
</div>
</div>
<p>After this step we can finally compute the statistic as follows:</p>
<div class="math notranslate nohighlight">
\[\chi ^{2}=\sum _{{i=1}}^{{r}}\sum _{{j=1}}^{{c}}{(O_{{i,j}}-E_{{i,j}})^{2} \over E_{{i,j}}}=N\sum _{{i,j}}p_{{i\cdot }}p_{{\cdot j}}\left({\frac  {(O_{{i,j}}/N)-p_{{i\cdot }}p_{{\cdot j}}}{p_{{i\cdot }}p_{{\cdot j}}}}\right)^{2}\]</div>
<p>We then choose a significance of 0.95 for our test (<span class="math notranslate nohighlight">\(\alpha = 0.5\)</span>), and draw the distribution with the significance level and the statistics value</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>

<span class="c1"># Choose the significance level and find the critical value</span>
<span class="n">prob</span> <span class="o">=</span> <span class="mf">0.95</span>
<span class="n">critical</span> <span class="o">=</span> <span class="n">chi2</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">dof</span><span class="p">)</span>

<span class="c1"># Build X axis</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="o">.</span><span class="mi">05</span><span class="p">)</span>

<span class="c1"># Draw the chi-squared pdf witht he appropriate dof</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">chi2</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">data_crosstab</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">data_crosstab</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.7</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Chi-Squared</span><span class="se">\n</span><span class="s1">   (dof=</span><span class="si">{</span><span class="n">dof</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="c1"># Draw the test statistics</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">chi_stat</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test-statistic&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">chi_stat</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">&quot;test statistic&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;p = </span><span class="si">{</span><span class="n">p</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>

<span class="c1"># Draw the line leaving behind 95% of the area under the curve</span>
<span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">critical</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;-.&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;critical-value&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">critical</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s2">&quot;critical value&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="s1">&#39;vertical&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;alpha = </span><span class="si">{</span><span class="mi">1</span><span class="o">-</span><span class="n">prob</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_13_0.png" src="_images/appendix_13_0.png" />
</div>
</div>
<p>Since our p-value is smaller than the <span class="math notranslate nohighlight">\(\alpha\)</span> we decided, we end up rejecting the <span class="math notranslate nohighlight">\(H_0\)</span>, hence, the two random variables are <strong>NOT</strong> independent.</p>
</div>
<div class="section" id="oversampling">
<span id="appendix-oversampling"></span><h2>3. Oversampling<a class="headerlink" href="#oversampling" title="Permalink to this headline">¶</a></h2>
<p>The two most common oversampling techniques are <strong>SMOTE</strong> and <strong>ADASYN</strong>.
Both techniques are based on the same oversampling algorithm:</p>
<div class="math notranslate nohighlight">
\[{\displaystyle x_{new} = x_i + \lambda \times (x_{zi} - x_i)}\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x_{new}\)</span> is the newly generated point</p></li>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span> is a point from the orginal dataset</p></li>
<li><p><span class="math notranslate nohighlight">\(x_{zi}\)</span> is the <span class="math notranslate nohighlight">\(z_{th}\)</span> nearest neighbor of <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
</ul>
<p>We are basically finding a new point as an interpolation of each point and one of it’s nearest neighbors.
The main issue is that approach only works with continuous features.
Since we are required to deal with continuous features <em>as well as</em> categorical features, we have to choose SMOTE, since ADASYN does not offer any solutions for the problem.</p>
<div class="section" id="smote-for-continuous-values">
<h3>3.1. <strong>SMOTE</strong> for continuous values<a class="headerlink" href="#smote-for-continuous-values" title="Permalink to this headline">¶</a></h3>
<p>As we just said, SMOTE for contiuous values is pretty straight forward:</p>
<ol class="simple">
<li><p>Determine the oversampling ratio for each point in the minority class</p></li>
<li><p>For each minority point:</p>
<ol class="simple">
<li><p>Find <span class="math notranslate nohighlight">\(k\)</span> nearest neighbors</p></li>
<li><p>Choose <span class="math notranslate nohighlight">\(n\)</span> of the <span class="math notranslate nohighlight">\(k\)</span> neighbors and compute the new point coordinates</p></li>
<li><p>Add the new points to the dataset</p></li>
</ol>
</li>
</ol>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create fake data</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.41</span><span class="p">,</span> <span class="mf">0.42</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.58</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mf">3.21</span><span class="p">,</span> <span class="mf">3.65</span><span class="p">,</span> <span class="mf">3.52</span><span class="p">,</span> <span class="mf">3.51</span><span class="p">,</span> <span class="mf">3.51</span><span class="p">,</span> <span class="mf">3.6</span><span class="p">,</span> <span class="mf">3.3</span><span class="p">]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Minority&#39;</span><span class="p">,</span> <span class="s1">&#39;Minority&#39;</span><span class="p">,</span> <span class="s1">&#39;Minority&#39;</span><span class="p">,</span> <span class="s1">&#39;Minority&#39;</span><span class="p">,</span> <span class="s1">&#39;Minority&#39;</span><span class="p">,</span> <span class="s1">&#39;Minority&#39;</span><span class="p">,</span> <span class="s1">&#39;Majority&#39;</span><span class="p">,</span> <span class="s1">&#39;Majority&#39;</span><span class="p">]</span>
<span class="n">a</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">l</span> <span class="o">=</span> <span class="mf">0.7</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="n">c</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="n">a</span><span class="p">})</span>

<span class="c1"># Build the new interpolated point</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
    <span class="p">{</span><span class="s1">&#39;x&#39;</span><span class="p">:</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span> <span class="o">*</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">]),</span>  
     <span class="s1">&#39;y&#39;</span><span class="p">:</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">l</span> <span class="o">*</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">]),</span>
     <span class="s1">&#39;c&#39;</span><span class="p">:</span> <span class="s1">&#39;New&#39;</span><span class="p">,</span>
     <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="n">size</span> <span class="o">=</span> <span class="mi">500000</span>
<span class="c1"># Draw the neighbourhood</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]</span><span class="o">==</span><span class="mf">0.4</span><span class="p">],</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Draw the interpolation line</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">5</span><span class="p">]],</span> <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">5</span><span class="p">]],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Draw points</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">)</span>

<span class="c1"># Some text</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span><span class="o">-</span><span class="mf">0.03</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="s2">&quot;$x_i$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="mf">0.035</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;$x_</span><span class="si">{zi}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">],</span> <span class="s2">&quot;$x_</span><span class="si">{new}</span><span class="s2">$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_16_0.png" src="_images/appendix_16_0.png" />
</div>
</div>
</div>
<div class="section" id="smote-for-categorical-values">
<h3>3.2. <strong>SMOTE</strong> for categorical values<a class="headerlink" href="#smote-for-categorical-values" title="Permalink to this headline">¶</a></h3>
<p>Since it is not possible to interpolate categorical features, we can extend SMOTE to deal with this kind of data.
The way the <em>imblearn</em> library does it is by adopting a <em>majority voting</em> technique among the k nearest neighbors of any given point.</p>
</div>
</div>
<div class="section" id="learning-theory">
<h2>4. Learning Theory<a class="headerlink" href="#learning-theory" title="Permalink to this headline">¶</a></h2>
<p>In order to understand the meaning behind some procedures like <em>Training-Test Data Split</em> or <em>Cross Validation</em> we must give a brief introduction to some basic concepts of learning theory.</p>
<div class="section" id="statistical-learning-framework">
<h3>4.1 Statistical Learning Framework<a class="headerlink" href="#statistical-learning-framework" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by defining some important terms that we will need in the following chapters:</p>
<ul class="simple">
<li><p>Learner: The Machine Learning algorithm</p></li>
<li><p>Domain set: The set of observations that we want to label <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> (in our case, the set of session recorded)</p></li>
<li><p>Label set: The set of possible labels <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> (in our case, <span class="math notranslate nohighlight">\(\{0, 1\}\)</span>)</p></li>
<li><p>Training data: A finite sequence of labeled domain points <span class="math notranslate nohighlight">\(\displaystyle S = \left( \left(  \mathcal{x}_1, \mathcal{y}_1 \right) \dots \left( \mathcal{x}_m, \mathcal{y}_m \right) \right)\)</span></p></li>
<li><p>Training data: A finite sequence of labeled domain points <span class="math notranslate nohighlight">\(S = \left( \left(  \mathcal{x}_1, \mathcal{y}_1 \right) \dots \left( \mathcal{x}_m, \mathcal{y}_m \right) \right)\)</span></p></li>
<li><p>Learner’s output: The prediction rule obtained by the learner, trained on Training data <span class="math notranslate nohighlight">\(\mathcal{h} = \mathcal{X} \rightarrow \mathcal{Y}\)</span></p></li>
<li><p>Data-Generating model: The probability distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> underlying <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and the <em>correct</em> labelling function <span class="math notranslate nohighlight">\(\mathcal{f}\ \text{s.t.}\ \mathcal{f}\left(\mathcal{x}_i\right)=\mathcal{y}_i\)</span></p></li>
<li><p>Measure of success: The error of a prediction rule <span class="math notranslate nohighlight">\(\mathcal{h} = \mathcal{X} \rightarrow \mathcal{Y}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[L_{\mathcal{D}, f}(h) \stackrel{\text { def }}{=} \underset{x \sim \mathcal{D}}{\mathbb{P}}[h(x) \neq f(x)] \stackrel{\text { def }}{=} \mathcal{D}(\{x: h(x) \neq f(x)\})\]</div>
<p>Meaning that the error of <span class="math notranslate nohighlight">\(\mathcal{h}\)</span> is the probability of randomly choosing an example <span class="math notranslate nohighlight">\(\mathcal{x}\)</span> for which the prediction rule disagrees from the labelling function.</p>
</div>
<div class="section" id="the-empirical-risk">
<h3>4.2 The Empirical Risk<a class="headerlink" href="#the-empirical-risk" title="Permalink to this headline">¶</a></h3>
<p>The goal of our learner (algorithm) is to find the best predictor <span class="math notranslate nohighlight">\(h\)</span>. The goodness of the predictor is measured by means of the <em>error</em>. The main problem is that the error we just defined depends on the true data distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and the labelling function <span class="math notranslate nohighlight">\(\mathcal{f}\)</span>, and both are not accessible by the learner.</p>
<p>All that our learner sees is the training set <em>S</em>, which works like a small window on the real world <span class="math notranslate nohighlight">\(D\)</span> that he will never able to experience. For this reason, we are only able to build an approximation of that error, we call it <em>Empirical Error (Risk)</em>
$<span class="math notranslate nohighlight">\(L_{S}(h) \stackrel{\text { def }}{=} \frac{\left|\left\{i \in[m]: h\left(x_{i}\right) \neq y_{i}\right\}\right|}{m}\)</span>$
That is, the ratio of examples in which our rule agrees with with the labelling function. Consequently, we train our learner by minimizing the <em>Empirical Risk</em>, a process called <strong>Empirical Risk Minimization (ERM)</strong></p>
</div>
<div class="section" id="overfitting">
<h3>4.3 Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">¶</a></h3>
<p>The <strong>ERM</strong> technique, although very intuitive, may lead to overfitting.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_inner_points</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n_outer_points</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="c1"># Outer boundary</span>
<span class="n">outer_circle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">outer_circle</span><span class="p">)</span>
<span class="c1"># Inner boundary</span>
<span class="n">inner_circle</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">fill</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">inner_circle</span><span class="p">)</span>

<span class="c1"># Plot outer samples</span>
<span class="n">angle_outer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outer_points</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
<span class="n">radius_outer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_outer_points</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">radius_outer</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_outer</span><span class="p">),</span> <span class="n">radius_outer</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_outer</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="c1"># Plot inner samples</span>
<span class="n">angle_inner</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_inner_points</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
<span class="n">radius_inner</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_inner_points</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">radius_inner</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angle_inner</span><span class="p">),</span> <span class="n">radius_inner</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angle_inner</span><span class="p">),</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;D&#39;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_21_0.png" src="_images/appendix_21_0.png" />
</div>
</div>
<p>Let’s imagine a bivariate dataset, where the <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is the uniform distribution in the blue circle, and the labelling function <span class="math notranslate nohighlight">\(\mathcal{f}\)</span> determines the label to be 1 if the sample falls inside the red circle, 0 otherwise. (Note that the area of the red circle is <span class="math notranslate nohighlight">\(\pi\)</span> while the area of the blue circle is <span class="math notranslate nohighlight">\(2\pi\)</span>)</p>
<p>Now consider the predictor:</p>
<div class="math notranslate nohighlight">
\[\begin{split}h_{S}(x)= \begin{cases}y_{i} &amp; \text { if } \exists i \in[m] \text { s.t. } x_{i}=x \\ 0 &amp; \text { otherwise. }\end{cases}\end{split}\]</div>
<p>Which essentially is remembering by heart the training samples, and guessing label 0 for every unknown sample. This predictor has <em>empirical risk</em> <span class="math notranslate nohighlight">\(L_{\mathcal{S}}(h_s) = 0\)</span>, for this reason it can be chosen by an ERM algorighm. But if we take a look at the <em>true risk</em> (we can do it only because we built the underlying distribution ourselves) is <span class="math notranslate nohighlight">\(L_{\mathcal{D}}(h_s) = \frac{1}{2}\)</span></p>
<p>We have found a predictor that performs perfectly on the training set, and very poorly on the true “world”.</p>
</div>
<div class="section" id="recrifying-overfitting">
<h3>4.4. Recrifying Overfitting<a class="headerlink" href="#recrifying-overfitting" title="Permalink to this headline">¶</a></h3>
<p>We have just shown that ERM is sensible to overfitting, we will now introduce some conditions to ERM that will guardantee us that if our predictor has good performance on the training set, it is <em>highly likely</em> to perform <em>well</em> over the underlying data distribution d.</p>
<div class="section" id="finite-hypothesis-classes">
<h4>4.4.1 Finite Hypothesis Classes<a class="headerlink" href="#finite-hypothesis-classes" title="Permalink to this headline">¶</a></h4>
<p>A common solution to overfitting is to limit the search space of the ERM. This means choosing an hypothesis class <em>in advance</em>, over which the ERM will look for our best predictor <em>h</em></p>
<div class="math notranslate nohighlight">
\[h_S = \operatorname{ERM}_{\mathcal{H}}(S) \in \underset{h \in \mathcal{H}}{\operatorname{argmin}} L_{S}(h)\]</div>
<p>We call <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> <em>Hypothesis class</em>, and our ERM now aims at finding the best <span class="math notranslate nohighlight">\(\mathcal{h} \in \mathcal{H}\)</span> that minimizes the <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span>. Although this reduces overfitting, we are practically steering our ERM towards a predefinet set of predictors, hence we are introducing a form of bias called <em>inductive bias</em>.</p>
</div>
<div class="section" id="realizability-assumption">
<h4>4.4.2 Realizability Assumption<a class="headerlink" href="#realizability-assumption" title="Permalink to this headline">¶</a></h4>
<div class="math notranslate nohighlight">
\[\exists \mathcal{h}^* \in \mathcal{h}\ \operatorname{s.t.}\ L_{\left(\mathcal{D}, \mathcal{f}\right)}\left(h^*\right)=0\]</div>
<p>This means that there always exists an hypothesis in our finite hypothesis set, for which the True Error is zero. This implies</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{D}^{\mathcal{m}}\left(S:L_S\left(h^*\right)=0\right)=1\)</span> with probability 1, over the random samples <span class="math notranslate nohighlight">\(S\)</span> sampled according to <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, labelled by <span class="math notranslate nohighlight">\(\mathcal{f}\)</span>, we have that the empirical error of <span class="math notranslate nohighlight">\(h^*\)</span> equals 0</p></li>
<li><p><span class="math notranslate nohighlight">\(L_S\left(h_S\right)=0\)</span> for every ERM hypothesis, the empirical risk equals 0</p></li>
</ol>
</div>
<div class="section" id="i-i-d-assumption">
<h4>4.4.3 I.I.D. Assumption<a class="headerlink" href="#i-i-d-assumption" title="Permalink to this headline">¶</a></h4>
<p>The samples in the training set are <em>independently and indentically distributed</em> according to <span class="math notranslate nohighlight">\(\mathcal{D}\)</span></p>
</div>
<div class="section" id="putting-it-all-together">
<h4>4.4.4 Putting it all together<a class="headerlink" href="#putting-it-all-together" title="Permalink to this headline">¶</a></h4>
<p>Since the training set is random, there is always a (small) probability that our sample is unrepresentative of the underlying <span class="math notranslate nohighlight">\(D\)</span>, We denote such probability as <span class="math notranslate nohighlight">\(\delta\)</span>, and call <span class="math notranslate nohighlight">\(\left(1-\delta\right)\)</span> the <em>confidence</em> of our prediction. Furthermore, since we can not guarantee a perfect prediction, we introduce the <em>accuracy</em> <span class="math notranslate nohighlight">\(\epsilon\)</span>. We now are able to define the failure of our learner as <span class="math notranslate nohighlight">\(L_{\left(\mathcal{D}, \mathcal{f}\right)}\left(h_S\right) &gt; \epsilon\)</span>.</p>
<p>We are now interested in finding an upper bound to the probability of sampling an m-tuple of instances that will lead to faliure in learning a good predictor. The quantity we want to upper bound is</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(\{ S|_x : L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon\}\right)\tag{1}\]</div>
<p>where <span class="math notranslate nohighlight">\(S|_x=\left(x_1, \dots, x_m\right)\)</span> are the instances of the training set.</p>
<p>Let:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{H_B}\)</span> be the set of “<em>bad</em>” hypothesis <span class="math notranslate nohighlight">\(\mathcal{H_B}=\{h \in \mathcal{H}:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon\}\)</span>. <span class="math notranslate nohighlight">\(\mathcal{H_B}\)</span> is the set of hypothesis that leads to learner’s failure (N.B. defined with respect to the <strong>True</strong> error).</p></li>
<li><p><span class="math notranslate nohighlight">\(M\)</span> be the set of “<em>misleading</em>” samples <span class="math notranslate nohighlight">\(M=\{S|_x:\exists h \in \mathcal{H}_B, L_S\left(h\right)=0\}\)</span>. <span class="math notranslate nohighlight">\(M\)</span> is the set of samples, for which a bad hypothesis (high <em>true</em> error) has a low <em>empirical</em> error.</p></li>
</ul>
<p>Recall:</p>
<ul class="simple">
<li><p>We are trying to bound <span class="math notranslate nohighlight">\(\mathcal{D^m}\left(\{ S|_x : L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon\}\right)\)</span></p></li>
</ul>
<p>Consider:</p>
<ul class="simple">
<li><p>The realizability assumption implies that: <span class="math notranslate nohighlight">\(L_S\left(h_S\right)=0\)</span></p></li>
</ul>
<p>This means that <span class="math notranslate nohighlight">\(L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon\)</span> can only happend if for some  <span class="math notranslate nohighlight">\(h \in \mathcal{H}_B\)</span> we have <span class="math notranslate nohighlight">\(L_S\left(h\right)=0\)</span> (this is because the bad hypothesis will be chosen via ERM only if it has empirical error equals 0 for realizability assumption). We can now say that the event above will only happen if our sample is in the set <span class="math notranslate nohighlight">\(M\)</span>.</p>
<p>We have shown that:
$<span class="math notranslate nohighlight">\(\{S|_x:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon \} \subseteq M\)</span>$
(meaning that the set of samples for which we have learner’s failure is bounded by the size of the set of misleading samples)</p>
<p>Hence:
$<span class="math notranslate nohighlight">\(\mathcal{D^m}\left(\{S|_x:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon \}\right) \leq \mathcal{D^m}\left(M\right)\tag{2}\)</span>$
(The probability of picking a sample that brings learner’s failure is bounded by the probability of picking a misleading sample)</p>
<p>Consider:</p>
<ul class="simple">
<li><p>The equation <span class="math notranslate nohighlight">\(M=\{S|_x:\exists h \in \mathcal{H}_B, L_S\left(h\right)=0\}=\underset{h\in\mathcal{H}_B}{\bigcup}\{S|_x:L_S\left(h\right)=0\}\)</span></p></li>
</ul>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(\{S|_x:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon \}\right)\leq\mathcal{D^m}\left(\underset{h\in\mathcal{H}_B}{\bigcup}\{S|_x:L_S\left(h\right)=0\}\right)\tag{3}\]</div>
<p>Consider:</p>
<ul class="simple">
<li><p>The union bound <span class="math notranslate nohighlight">\(\mathcal{D}\left(A \cup B\right) \leq \mathcal{D}\left(A\right) +\mathcal{D}\left(B\right)\)</span></p></li>
</ul>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(\{S|_x:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon \}\right) \leq \underset{h\in\mathcal{H}_B}\sum{\mathcal{D^m}\left(S|_x:L_S\left(h\right)=0\right)}\tag{4}\label{eq:4}\]</div>
<p>Now, let’s bound each single summand on the right side.</p>
<p>Fix:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h \in \mathcal{H_B}\)</span></p></li>
</ul>
<p>Consider:</p>
<ul class="simple">
<li><p>The event <span class="math notranslate nohighlight">\(L_S\left(h\right)=0\)</span> indicates the case where our predictor <span class="math notranslate nohighlight">\(h\)</span> always agrees with the true labelling <span class="math notranslate nohighlight">\(f\)</span>, formally <span class="math notranslate nohighlight">\(\forall{i}, h\left(x_i\right)=\mathcal{f}\left(x_i\right)\)</span></p></li>
</ul>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(S|_x:L_S\left(h\right)=0\right)=\mathcal{D^m}\left(\{S|_x:\forall{i}, h\left(x_i\right)=\mathcal{f}\left(x_i\right)\right\})\]</div>
<p>Consider:</p>
<ul class="simple">
<li><p>The training samples are i.i.d.</p></li>
</ul>
<p>Then</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(\{S|_x:\forall{i}, h\left(x_i\right)=\mathcal{f}\left(x_i\right)\right\})=\underset{i=1}{\overset{m}{\prod}} \mathcal{D}\left(\{\mathcal{x}_i:h\left(\mathcal{x}_i\right)=\mathcal{f}\left(\mathcal{x}_i\right)\}\right)\]</div>
<p>Remember:</p>
<ul class="simple">
<li><p>The definition of true risk: <span class="math notranslate nohighlight">\(L_{\mathcal{D}, f}(h) \stackrel{\text { def }}{=} \mathcal{D}(\{x: h(x) \neq f(x)\})\)</span></p></li>
</ul>
<p>Then, for each individual sampling of an element of the training set we have</p>
<div class="math notranslate nohighlight">
\[\mathcal{D}\left(\{\mathcal{x_i}:h\left(\mathcal{x_i}\right)=\mathcal{y_i}\}\right) = 1 - L_\left(\mathcal{D}, \mathcal{f}\right)\left(h\right) \leq 1 - \epsilon\]</div>
<p>Where the last inequation follows from the fact that our <span class="math notranslate nohighlight">\(h \in \mathcal{H_B}\)</span></p>
<p>Consider:</p>
<ul class="simple">
<li><p>the inequality <span class="math notranslate nohighlight">\(\left(1 - \epsilon\right)^m \leq e^{-\epsilon m}\)</span></p></li>
</ul>
<p>We now have proved that:</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(S|_x:L_S\left(h\right)=0\right) \leq (1-e)^m \leq e^{-\epsilon m}\]</div>
<p>Now, sostituting back in equation <span class="math notranslate nohighlight">\(\left(\ref{eq:4}\right)\)</span> we get</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(\{S|_x:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon \}\right) \leq \left|\mathcal{H_B}\right|e^{-\epsilon m}\tag{5}\]</div>
<p>Consider:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\left|\mathcal{H_B}\right| \leq \left|\mathcal{H}\right|\)</span></p></li>
</ul>
<p>We conclude that</p>
<div class="math notranslate nohighlight">
\[\mathcal{D^m}\left(\{S|_x:L_{\left(\mathcal{D},\mathcal{f}\right)}\left(h_S\right)&gt;\epsilon \}\right) \leq \left|\mathcal{H}\right|e^{-\epsilon m}\tag{6}\]</div>
<p><strong>—</strong></p>
<p>Let</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{H} be a finite hypothesis class\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\delta \in \left(0, 1\right) \text{and} \epsilon &gt; 0\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> be an integer that satisfies <span class="math notranslate nohighlight">\(m \geq \frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\)</span></p></li>
</ul>
<p>Then, for any labeling function <span class="math notranslate nohighlight">\(\mathcal{f}\)</span> and for any distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> for which the realizability assumption holds, with probability at least <span class="math notranslate nohighlight">\(1 - \delta\)</span> over the choice of an i.i.d. sample <span class="math notranslate nohighlight">\(S\)</span> of size <span class="math notranslate nohighlight">\(m\)</span>, we have that for every <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> hypothesis <span class="math notranslate nohighlight">\(h_S\)</span>, it holds that</p>
<div class="math notranslate nohighlight">
\[L_{\left(\mathcal{D}, \mathcal{f}\right)}\left(H_S\right)\leq \epsilon\]</div>
<p>The above statements mean that, for a training set of size <span class="math notranslate nohighlight">\(m\)</span> (<em>big enough</em>) and a <strong>finite</strong> hypothesis set <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, the <span class="math notranslate nohighlight">\(\operatorname{ERM_{\mathcal{H}}}\)</span> rule will be <em>probably</em> (with confindence <span class="math notranslate nohighlight">\(1 - \delta\)</span>) <em>approximately</em> (up to an error <span class="math notranslate nohighlight">\(\epsilon\)</span>) correct.</p>
</div>
</div>
<div class="section" id="pac-learning">
<h3>4.5. PAC Learning<a class="headerlink" href="#pac-learning" title="Permalink to this headline">¶</a></h3>
<p>In the paragraph above we found out that for a finite hypothesis class, if the sample size <span class="math notranslate nohighlight">\(m\)</span> of the training set <span class="math notranslate nohighlight">\(S\)</span> that we feed to the <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> in order to find our hypothesis <span class="math notranslate nohighlight">\(h_S\)</span> is <strong>big enough</strong>, then our <span class="math notranslate nohighlight">\(h_S\)</span> will be <strong>probably approximately correct</strong>.</p>
<p>We can now define PAC Learnability as follows:</p>
<div class="important admonition">
<p class="admonition-title">PAC Learning</p>
<p>A hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is PAC learnable if there exists a function <span class="math notranslate nohighlight">\(m_{\mathcal{H}}:(0,1)^{2} \rightarrow \mathbb{N}\)</span> and a learning algorithm that has the following properties <span class="math notranslate nohighlight">\(\forall \epsilon,\delta\in\left(0, 1\right), \forall \mathcal{D}\sim\mathcal{X}, \forall \mathcal{f}:\mathcal{X}\rightarrow\left(0, 1\right)\)</span> if the realizability assumption holds w.r.t. <span class="math notranslate nohighlight">\(\mathcal{H,D,f}\)</span> then when running the learning algorithm on <span class="math notranslate nohighlight">\(m \geq m_\mathcal{H}\left(\epsilon,\theta\right)\)</span> i.i.d. samples generated by <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and labelled by <span class="math notranslate nohighlight">\(\mathcal{f}\)</span> the algorighm returns a hypothesis <span class="math notranslate nohighlight">\(h\)</span> such that with probability of at least <span class="math notranslate nohighlight">\(1-\theta\)</span> (probably) <span class="math notranslate nohighlight">\(L_{\left(\mathcal{D}, \mathcal{f}\right)}\left(h\right)\leq\epsilon\)</span> (approximately correct)</p>
</div>
<p>It is important to note that if <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is PAC learnable, there exists mode than 1 function <span class="math notranslate nohighlight">\(m_H\)</span> that satisfies the requirements for PAC learnability. Therefore, we will redefine the sample complexity to be the minimal integer that satisfies the requirements of PAC learning:</p>
<div class="math notranslate nohighlight">
\[m_{\mathcal{H}}(\epsilon, \delta) \leq\left\lceil\frac{\log (|\mathcal{H}| / \delta)}{\epsilon}\right\rceil\]</div>
</div>
<div class="section" id="generalizing-the-model-removing-the-realizability-assumption">
<h3>4.6 Generalizing the model - Removing the Realizability Assumption<a class="headerlink" href="#generalizing-the-model-removing-the-realizability-assumption" title="Permalink to this headline">¶</a></h3>
<p>The first step towards the generalization of the model is to remove the realizability assumption. Remember, the realizabilty assumption says that <span class="math notranslate nohighlight">\(\exists h^*\in\mathcal{H}:\mathcal{D}\left(\{h_*\left(\mathcal{x}\right)=\mathcal{f}\left(\mathcal{x}\right)\}\right)=1\)</span>. In many real problems, this is not the case. Furthermore, it is also unrealistic to suppose that the labels are fully determined by the labelling function <span class="math notranslate nohighlight">\(\mathcal{f}\)</span>.</p>
<p>For this reason, from now on, we indicate the probablity distribution over <span class="math notranslate nohighlight">\(\mathcal{X} \times \mathcal{Y}\)</span>, were</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is our domain set as before, <span class="math notranslate nohighlight">\(\mathcal{D_x}\)</span> is the marginal over the unlabeled domain points</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is a set of labels, <span class="math notranslate nohighlight">\(\mathcal{D\left(\left(x, y\right)|x\right)}\)</span> is the conditional over the labels for each domain point</p></li>
</ul>
<p>This makes <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> a <em>joint probability distribution</em> over the domain points and the labels.</p>
<p>We can now rewrite some of the concept that we defined previously:</p>
<ul class="simple">
<li><p><strong>True Error</strong>: <span class="math notranslate nohighlight">\(L_\mathcal{D}\left(h\right) \overset{\operatorname{def}}{=} \mathcal{D}\left(\{\left(\mathcal{x, y}\right): h\left(\mathcal{x}\right)\neq\mathcal{y}\}\right)\)</span></p></li>
<li><p><strong>Empirical Error</strong>: <span class="math notranslate nohighlight">\(L_{S}(h) \stackrel{\text { def }}{=} \frac{\left|\left\{i \in[m]: h\left(x_{i}\right) \neq y_{i}\right\}\right|}{m}\)</span></p></li>
</ul>
<p>We can now replace the realizability assumption by considering the following example:</p>
<p>Given a probability distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> over <span class="math notranslate nohighlight">\(\mathcal{X} \times \{0, 1\}\)</span>, the best label predicting function from <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}f_{\mathcal{D}}(x)= \begin{cases}1 &amp; \text { if } \mathbb{P}[y=1 \mid x] \geq 1 / 2 \\ 0 &amp; \text { otherwise }\end{cases}\end{split}\]</div>
<p>The above classifier is called <em>Bayes classifier</em>, and it is the optimal predictor for every <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, formally</p>
<div class="math notranslate nohighlight">
\[\forall g \ L_\mathcal{D}\left(\mathcal{f_D}\right)\leq L_\mathcal{D}\left(\mathcal{g}\right)\]</div>
<p>However, the <em>Bayes Risk</em> can not once more be computed, as it depends on <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. Since we can not hope to find a predictor that has a smaller error than the minimal possible (the one of the Bayes classifier), we require that the learning algorithm will find a predictor whose ierror is <em>not much larger</em> than the best possible error of a predictor <em>in some given benchmark hypothesis class</em></p>
<div class="important admonition">
<p class="admonition-title">Agnostic PAC Learning</p>
<p>A hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is <strong>agnostic</strong> PAC learnable if there exists a function <span class="math notranslate nohighlight">\(m_{\mathcal{H}}:(0,1)^{2} \rightarrow \mathbb{N}\)</span> and a learning algorithm that has the following property: <span class="math notranslate nohighlight">\(\forall \epsilon,\theta\in\left(0, 1\right), \forall \mathcal{D}\sim\mathcal{X}\times\mathcal{Y}\)</span>, when running the learning algorithm on <span class="math notranslate nohighlight">\(m \geq m_\mathcal{H}\left(\epsilon,\delta\right)\)</span> i.i.d. samples generated by <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> the algorighm returns a hypothesis <span class="math notranslate nohighlight">\(h\)</span> such that, with probability of at least <span class="math notranslate nohighlight">\(1-\delta\)</span></p>
<div class="math notranslate nohighlight">
\[L_\mathcal{D}\left(h\right)\leq \underset{h^\prime\in\mathcal{H}}{\operatorname{min}} L_\mathcal{D}\left(h^\prime\right) +\epsilon\]</div>
</div>
<p>With the above definition we have relaxed the PAC learnable property, by removing the requirement for the learner to have a small error in absolute terms, and compared it to the best learner among the class of predictors that we choose.</p>
</div>
<div class="section" id="generalizing-the-model-extending-further-than-binary-classification">
<h3>4.7 Generalizing the model - Extending further than Binary Classification<a class="headerlink" href="#generalizing-the-model-extending-further-than-binary-classification" title="Permalink to this headline">¶</a></h3>
<p>The next relaxation we are going to perform over our model concerns the scope of the Learning Problem. To give an idea of what it means, let’s consider some examples of different learning tasks:</p>
<ul class="simple">
<li><p>Multiclass Classification: We do not require for our classification to be binary. We could think of document classification, where our <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> is the set of all documents, and our <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span> is a large but finite set (e.g. Sport, Polithics, Kitchen, News). Our training sample still looks exactly the same as previously discussed, a finite sequence of pairs <span class="math notranslate nohighlight">\(\mathcal{(x_i, y_i)}\)</span>, and the learner’s output will still be a function <span class="math notranslate nohighlight">\(h_S:\mathcal{X}\rightarrow\mathcal{Y}\)</span>. As a metric we can use the probability, over pairs, that our predictor suggests a wrong label.</p></li>
<li><p>Regression: In regression, the goal is to find a functional relationship between <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>. For example, one could want to find a linear function that predicts a child’s weight at birth give some ultrasound measurements (e.g. head circumference, abdominal circumference and femur length). Our <span class="math notranslate nohighlight">\(\mathcal{X} \in \mathbb{R}^3\)</span> and the labels set, is the set of real numbers. Our training data still looks the same even in this task (a pair <span class="math notranslate nohighlight">\(\mathcal{\left(x_i, y_i\right)}\)</span>). However, the measure of success differs. We may evaluate the qualityb of a hypothesis function <span class="math notranslate nohighlight">\(h_S:\mathcal{X}\rightarrow\mathcal{Y}\)</span> using the <em>expected square difference</em> between the true labels and the predictor’s outputs:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[L_\mathcal{D}\left(h\right) \overset{\operatorname{def}}{=} \underset{\mathcal{\left(x, y\right)\sim D}}{\mathbb{E}} \left(h\left(\mathcal{x}\right)-\mathcal{y}\right)^2\]</div>
<p>For this reason, we generalize our measure of success as follows:</p>
<div class="important admonition">
<p class="admonition-title">Generalized Loss Function</p>
<p>Given any set <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> and some domain <span class="math notranslate nohighlight">\(Z\)</span> let <span class="math notranslate nohighlight">\(l\)</span> be any function <span class="math notranslate nohighlight">\(\ell:\mathcal{H}\times Z \rightarrow \mathbb{R}_+\)</span> we call such functions <em>loss functions</em>. (for classification problems, <span class="math notranslate nohighlight">\(Z=\mathcal{Z\times Y}\)</span>)</p>
</div>
<p>We can again rewrite the definitions of true and empirical risk</p>
<p>We can now rewrite some of the concept that we defined previously:</p>
<ul class="simple">
<li><p><strong>True Risk</strong>: <span class="math notranslate nohighlight">\(L_\mathcal{D}\left(h\right) \overset{\operatorname{def}}{=} \underset{\mathcal{z\sim D}}{\mathbb{E}}\left[\ell\left(\mathcal{h, z}\right)\right]\)</span> (The risk is the expected value of the loss)</p></li>
<li><p><strong>Empirical Error</strong>: <span class="math notranslate nohighlight">\(L_{S}(h) \overset{\operatorname{def}}{=} \frac{1}{m} \overset{m}{\underset{i=1}{\sum}}\ell\mathcal{\left(h,z_i\right)}\)</span> where <span class="math notranslate nohighlight">\(S=\mathcal{\left(z_1,\dots,z_m\right)\in Z_m}\)</span></p></li>
</ul>
<p>Let’s now write the loss for classification and regression tasks:</p>
<ul class="simple">
<li><p><strong>0-1 Loss</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\ell_{0-1}(h,(x, y)) \stackrel{\text { def }}{=} \begin{cases}0 &amp; \text { if } h(x)=y \\ 1 &amp; \text { if } h(x) \neq y\end{cases}\end{split}\]</div>
<ul class="simple">
<li><p><strong>Square Loss</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\ell_{sq}\left(h, \left(\mathcal{x, y}\right)\right) \overset{\operatorname{def}}{=} \left(h\left(\mathcal{x}\right)-\mathcal{y}\right)^2\]</div>
<p>To summarize:</p>
<div class="important admonition">
<p class="admonition-title">Agnostic PAC Learning for Genearl Loss Functions</p>
<p>A hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is <strong>agnostic</strong> PAC learnable with respect to a set <span class="math notranslate nohighlight">\(Z\)</span> and a loss function <span class="math notranslate nohighlight">\(\ell:\mathcal{H}\times Z\rightarrow \mathbb{R}_+\)</span> if there exists a function <span class="math notranslate nohighlight">\(m_{\mathcal{H}}:(0,1)^{2} \rightarrow \mathbb{N}\)</span> and a learning algorithm that has the following property: <span class="math notranslate nohighlight">\(\forall \epsilon,\theta\in\left(0, 1\right), \forall \mathcal{D}\sim&gt;\)</span>, when running the learning algorithm on <span class="math notranslate nohighlight">\(m \geq m_\mathcal{H}\left(\epsilon,\delta\right)\)</span> i.i.d. samples generated by <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> the algorighm returns a hypothesis <span class="math notranslate nohighlight">\(h\)</span> such that, with probability of at least <span class="math notranslate nohighlight">\(1-\delta\)</span> (over
the choice of the m training examples)</p>
<div class="math notranslate nohighlight">
\[L_\mathcal{D}\left(h\right)\leq \underset{h^\prime\in\mathcal{H}}{\operatorname{min}} L_\mathcal{D}\left(h^\prime\right) +\epsilon\]</div>
<p>where <span class="math notranslate nohighlight">\(L_\mathcal{D}\left(h\right) = \mathbb{E}_{\mathcal{z\sim D}}\left[\ell\left(\mathcal{h, z}\right)\right]\)</span></p>
</div>
</div>
<div class="section" id="uniform-convergence">
<h3>4.8 Uniform Convergence<a class="headerlink" href="#uniform-convergence" title="Permalink to this headline">¶</a></h3>
<p>Recall the <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> paradigm given a hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>:</p>
<ol class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> recieves a training sample <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>The learner evaluates the <em>empirical</em> risk of each <span class="math notranslate nohighlight">\(h \in\mathcal{H}\)</span> over the training sample <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>The learner outputs an <span class="math notranslate nohighlight">\(h\)</span> that minimizes the <em>empirical</em> risk.</p></li>
</ol>
<p>We then hope that this <span class="math notranslate nohighlight">\(h_S\)</span> is also a risk minimizer with respect to the <em>true data probability distribution</em> <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>. For that, it is enough to ensure that the empricial risk of all members of <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> are good approximations of their true risk.</p>
<div class="important admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(\epsilon\)</span>-representative sample</p>
<p>A training set <span class="math notranslate nohighlight">\(S\)</span> is called <span class="math notranslate nohighlight">\(\epsilon\)</span>-representative (w.r.t. domain <span class="math notranslate nohighlight">\(Z\)</span>, hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span>, loss function <span class="math notranslate nohighlight">\(\ell\)</span>, and distribuyion <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>) if</p>
<div class="math notranslate nohighlight">
\[\forall{h}\in\mathcal{H},\ \left|L_S\left(h\right)-L_D\left(h\right)\right|\leq{\epsilon}\]</div>
</div>
<p>Now assume that a training set <span class="math notranslate nohighlight">\(S\)</span> is <span class="math notranslate nohighlight">\(\frac{\epsilon}{2}\)</span>-representative, then any output of <span class="math notranslate nohighlight">\(\operatorname{ERM}_\mathcal{H}\left(S\right)\)</span>, namely, any <span class="math notranslate nohighlight">\(h_S\in \underset{h\in\mathcal{H}}{\operatorname{argmin}}L_S\left(h\right)\)</span> satisfies</p>
<div class="math notranslate nohighlight">
\[L_D\left(h_S\right) \leq \underset{h\in\mathcal{H}}{\operatorname{min}} L_D\left(h\right) + \epsilon\]</div>
<p>Proof:</p>
<div class="math notranslate nohighlight">
\[L_{\mathcal{D}}\left(h_{S}\right) \overset{1}{\leq} L_{S}\left(h_{S}\right)+\frac{\epsilon}{2} \overset{2}{\leq} L_{S}(h)+\frac{\epsilon}{2} \overset{3}{\leq} L_{\mathcal{D}}(h)+\frac{\epsilon}{2}+\frac{\epsilon}{2}=L_{\mathcal{D}}(h)+\epsilon,\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(3\)</span> hold because <span class="math notranslate nohighlight">\(S\)</span> is <span class="math notranslate nohighlight">\(\epsilon\)</span>-representative: <span class="math notranslate nohighlight">\(\left|L_S\left(h\right)-L_D\left(h\right)\right|\leq{\epsilon}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(2\)</span> holds because <span class="math notranslate nohighlight">\(h_S\)</span> is an <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> predictor (the hypothesis which yields the minimum empirical risk)</p></li>
</ul>
<p>We can now say that to ensure that the <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> rule is an agnorstic PAC learner, it is enough to show that with probability of at least <span class="math notranslate nohighlight">\(1-\theta\)</span> over the random choice of a training set, it will be an <span class="math notranslate nohighlight">\(\epsilon\)</span>-representative training set.</p>
<p>As we introduced <span class="math notranslate nohighlight">\(m_\mathcal{H}\left(\delta, \epsilon\right)\)</span> for PAC learning, which told us the minimum sample complexity needed in order to <strong>approximately</strong> have (with confidence <span class="math notranslate nohighlight">\(1-\theta\)</span>) a <strong>correct</strong> learner (up to an error <span class="math notranslate nohighlight">\(\epsilon\)</span>), we now introduce</p>
<div class="math notranslate nohighlight">
\[m^{UC}_\mathcal{H}:\left(0, 1\right)^2\rightarrow\mathbb{N}\]</div>
<div class="important admonition">
<p class="admonition-title">Uniform Convergence</p>
<p>We say that a hypothesis class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> has the <em>uniform convergence</em> property (w.r.t a domain <span class="math notranslate nohighlight">\(Z\)</span> and a loss function <span class="math notranslate nohighlight">\(\ell\)</span>) if there exists a function <span class="math notranslate nohighlight">\(m^{UC}_\mathcal{H}:\left(0, 1\right)^2\rightarrow\mathbb{N}\)</span> such that for every <span class="math notranslate nohighlight">\(\epsilon,\delta\in\left(0, 1\right)\)</span> and for every probability distribution <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> over <span class="math notranslate nohighlight">\(Z\)</span>, if <span class="math notranslate nohighlight">\(S\)</span> is a sample of <span class="math notranslate nohighlight">\(m \geq m^{UC}_\mathcal{H}\left(\epsilon,\theta\right)\)</span> examples, drawn i.i.d. according to <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, then, with probability of at least <span class="math notranslate nohighlight">\(1-\delta\)</span>, <span class="math notranslate nohighlight">\(S\)</span> is <span class="math notranslate nohighlight">\(\epsilon\)</span>-representative (which in turn means that the <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> rule is an agnostic PAC learner)</p>
</div>
<p>it follows that if a class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> has the uniform convergence property (meaning that for each <span class="math notranslate nohighlight">\(h\in\mathcal{H}\)</span> the there exists a function that gives us the minimum sample complexity that, with proability <span class="math notranslate nohighlight">\(1-\delta\)</span> over the sampling of the domain set, gives us a training set <span class="math notranslate nohighlight">\(S\)</span> which is <span class="math notranslate nohighlight">\(\epsilon\)</span>-representative) with a function <span class="math notranslate nohighlight">\(m^{UC}_\mathcal{H}\)</span> then the class is agnostically PAC learnable with sample complexity <span class="math notranslate nohighlight">\(m_\mathcal{H}\left(\epsilon,\delta\right) \leq m^{UC}_\mathcal{H}\left(\epsilon /2, \delta\right)\)</span>. Furthermore, in that case, the <span class="math notranslate nohighlight">\(\operatorname{ERM}_\mathcal{H}\)</span> paradigm is a succesful agnostic PAC learner for <span class="math notranslate nohighlight">\(\mathcal{H}\)</span></p>
</div>
<div class="section" id="recap">
<h3>4.8 Recap<a class="headerlink" href="#recap" title="Permalink to this headline">¶</a></h3>
<p>Below is a recap of the most important results achieved during this brief introduction</p>
<p><img alt="resume" src="_images/pac.png" /></p>
</div>
<div class="section" id="vc-dimension">
<h3>4.8 VC-dimension<a class="headerlink" href="#vc-dimension" title="Permalink to this headline">¶</a></h3>
<p>The <em>Vapnik-Chervonenkis</em>(VC)-dimension is a property of a hypothesis class that gives the correct characterization of its learnability. In order to define the VC-dimension we first need to define a preliminary notion called <em>shattering</em></p>
<div class="important admonition">
<p class="admonition-title">Shattering</p>
<p>Let <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> be a class of <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> functions over some domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> and let <span class="math notranslate nohighlight">\(A\subseteq\mathcal{X}\)</span>. We say that <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> shatters <span class="math notranslate nohighlight">\(A\)</span> if <span class="math notranslate nohighlight">\(\forall \mathcal{g}:A\rightarrow\{0, 1\} \exists h\in\mathcal{H}\ \operatorname{s.t.}\ \forall \mathcal{x} \in A,\ h\left(\mathcal{x}\right) = g\left(\mathcal{x}\right)\)</span></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">patches</span>
<span class="kn">import</span> <span class="nn">matplotlib.path</span> <span class="k">as</span> <span class="nn">mpath</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">)</span>

<span class="c1"># fig = plt.figure()</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Domain X</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">patches</span><span class="o">.</span><span class="n">Rectangle</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">8.5</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="s1">&#39;$\mathcal</span><span class="si">{X}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="c1"># Subset A</span>
<span class="n">ax</span><span class="o">.</span><span class="n">add_patch</span><span class="p">(</span><span class="n">patches</span><span class="o">.</span><span class="n">Circle</span><span class="p">((</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">ec</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">6.4</span><span class="p">,</span> <span class="s1">&#39;$A$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>

<span class="c1"># 0-1 behaviour over A</span>
<span class="n">x_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">);</span>
<span class="n">y_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_a</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span><span class="o">*</span><span class="mf">0.2</span> <span class="o">+</span> <span class="mi">5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_a</span><span class="p">,</span> <span class="n">y_a</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">4.9</span><span class="p">,</span> <span class="mf">5.6</span><span class="p">,</span> <span class="s1">&#39;$\mathcal</span><span class="si">{g}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>

<span class="c1"># 0-1 behaviour over X</span>
<span class="n">x_x_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y_x_left</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_x_left</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span><span class="o">*</span><span class="mf">0.5</span> <span class="o">+</span> <span class="mi">5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_x_left</span><span class="p">,</span> <span class="n">y_x_left</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">x_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">);</span>
<span class="n">y_a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_a</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span><span class="o">*</span><span class="mf">0.2</span> <span class="o">+</span> <span class="mi">5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_a</span><span class="p">,</span> <span class="n">y_a</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">x_x_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">y_x_right</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x_x_right</span><span class="o">*</span><span class="p">(</span><span class="mi">4</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">))</span><span class="o">*</span><span class="mf">0.5</span> <span class="o">+</span> <span class="mi">5</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_x_right</span><span class="p">,</span> <span class="n">y_x_right</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">2.8</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;$\mathcal</span><span class="si">{h}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">)</span>
<span class="c1"># Amplitude of the sine wave is sine of a variable like time</span>


<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_25_0.png" src="_images/appendix_25_0.png" />
</div>
</div>
<p>In the figure above we can see a (green) function <span class="math notranslate nohighlight">\(\mathcal{g}:A\rightarrow\{0, 1\}\)</span> that represents a <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> behaviour over <span class="math notranslate nohighlight">\(A\)</span>. And a function <span class="math notranslate nohighlight">\(h\)</span> defined over the domain <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> that agrees with the function <span class="math notranslate nohighlight">\(g\)</span> over the set of points <span class="math notranslate nohighlight">\(A\)</span>.</p>
<div class="note admonition">
<p class="admonition-title">Meaning of <span class="math notranslate nohighlight">\({0, 1}\)</span> behaviour</p>
<p><span class="math notranslate nohighlight">\(\{0,1\}\)</span> behaviour in this settings means “<em>a way of splitting the set A in two subsets</em>”</p>
</div>
<div class="note admonition">
<p class="admonition-title">Function-Set duality</p>
<p>There is equivalence between functions <span class="math notranslate nohighlight">\(\mathcal{h}:\mathcal{X}\rightarrow\{0, 1\}\)</span> and subsets <span class="math notranslate nohighlight">\(A\subseteq\mathcal{X}\)</span>. Given any <span class="math notranslate nohighlight">\(\mathcal{h}\)</span> I can define a set</p>
<div class="math notranslate nohighlight">
\[\forall{h}:\mathcal{X}\rightarrow\{0, 1\}\exists A_{\mathcal{h}}=\{\mathcal{x}\in\mathcal{X}:\mathcal{h}\left(\mathcal{x}\right)=1\}\]</div>
<p>The relationship also works in the opposite direction:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\forall{A}\subseteq\mathcal{X}\ \exists h_A\left(\mathcal{x}\right)=\begin{cases}1 &amp; \text { if } x\in A \\ 0 &amp; \text { if } x \notin A \end{cases}\end{split}\]</div>
</div>
<p>We can now define the VC-Dimension</p>
<div class="important admonition">
<p class="admonition-title">Vapnik-Chervonenkis dimension</p>
<p>The VC-dimension of a class <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is defined as the size of the maximal shattered set.
$<span class="math notranslate nohighlight">\(\operatorname{VCdim}\left(\mathcal{H}\right)\overset{\operatorname{def}}{=}\underset{A \operatorname{shattered by} \mathcal{H}}{\operatorname{max}} \left\lvert A\right\rvert \)</span>$</p>
</div>
<div class="note admonition">
<p class="admonition-title">How to compute VC of <span class="math notranslate nohighlight">\(\mathcal{H}\)</span></p>
<ul class="simple">
<li><p>If I claim that <span class="math notranslate nohighlight">\(\operatorname{VCdim}\left(\mathcal{H}\right)\geq n\)</span> then I have to find at least 1 set of size <span class="math notranslate nohighlight">\(n\)</span> which is shattered</p></li>
<li><p>If I claim that <span class="math notranslate nohighlight">\(\operatorname{VCdim}\left(\mathcal{H}\right)&lt; n\)</span> then I have to show that no matter how I pick <span class="math notranslate nohighlight">\(n\)</span> points, they will never be shattered by <span class="math notranslate nohighlight">\(\mathcal{h}\)</span></p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="k-fold-cross-validation">
<h2>5. K-Fold Cross Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>The k-fold cross validation technique is designed to provide an accurate estimate of the true error without wasting too much data (as it happens in regular validation techniques). This method consists in dividng the original training dataset of size <span class="math notranslate nohighlight">\(m\)</span>, into <span class="math notranslate nohighlight">\(k\)</span> subsets of size <span class="math notranslate nohighlight">\(m/k\)</span>. Once for each fold, a fold gets removed from the starting set and used as validation, while the remaining <span class="math notranslate nohighlight">\(k-1\)</span> are used to train the model. Finally, the average of the empirical error computed on each fold is computed and considered as the average estimate of the true error. Usually, k-fold is used to perform model selection, and once the final model is found, it is then re-trained on the whole training set.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="model_exploration.html" title="previous page">Model Exploration</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nicola Occelli<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>