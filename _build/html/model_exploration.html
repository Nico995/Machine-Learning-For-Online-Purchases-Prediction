
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Model Exploration &#8212; Predicting Shopper Intentions</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Appendix" href="appendix.html" />
    <link rel="prev" title="Pipelines" href="data_preparation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo-resized.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Predicting Shopper Intentions</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro.html">
   Predicting Shopper Intentions
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="data_exploration.html">
   Data Exploration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="data_preparation.html">
   Pipelines
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Model Exploration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="appendix.html">
   Appendix
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/model_exploration.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fmodel_exploration.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/model_exploration.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#support-vector-machines">
   1. Support Vector Machines
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#theory">
     1.1 Theory
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#margin-and-hard-svm">
       1.1.1 Margin and Hard-SVM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#soft-svm">
       1.1.2 Soft-SVM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#kernel-trick">
       1.1.3 Kernel Trick
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#application">
     1.2 Application
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#trees-and-random-forests">
   2. Trees and Random Forests
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#growing-a-tree">
     2.1. Growing a Tree
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-trees-to-forests">
     2.2. From Trees to Forests
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bagging-and-cv">
     2.3 Bagging and CV
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     2.4 Application
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="model-exploration">
<span id="chapters-model-exploration"></span><h1>Model Exploration<a class="headerlink" href="#model-exploration" title="Permalink to this headline">¶</a></h1>
<p>This section is <strong>not</strong> called model selection because our final goal is not to find the model with the best average of the true error estimation. We instead aim to showcase the common Data Science pipeline and try multiple models, showing at the same time the programming side and the mathematical side of them.</p>
<div class="section" id="support-vector-machines">
<h2>1. Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">¶</a></h2>
<p>This algorithm tries to find a hyperplane that divides the saples living in the feature space, maximizing the minimum distance between the hyperplane itself and the closest point in both resulting halfspaces.</p>
<div class="section" id="theory">
<h3>1.1 Theory<a class="headerlink" href="#theory" title="Permalink to this headline">¶</a></h3>
<div class="section" id="margin-and-hard-svm">
<h4>1.1.1 Margin and Hard-SVM<a class="headerlink" href="#margin-and-hard-svm" title="Permalink to this headline">¶</a></h4>
<p>Let <span class="math notranslate nohighlight">\(S = \left(\mathbf{x}_1, \mathcal{y}_1\right)\dots\left(\mathbf{x}_m, \mathcal{y}_m\right)\)</span> be the training set of size m, where each <span class="math notranslate nohighlight">\(\mathbf{x}_i\in\mathbb{R}^d\)</span> and <span class="math notranslate nohighlight">\(\mathcal{y}_i\in\{\pm1\}\)</span>. We say that the training set is linearelt separable, if there exists a halfspace, <span class="math notranslate nohighlight">\(\left(\mathbf{w}, \mathcal{b}\right)\)</span> such that <span class="math notranslate nohighlight">\(y_{i}=\operatorname{sign}\left(\left\langle\mathbf{w}, \mathbf{x}_{i}\right\rangle+b\right)\)</span> for all <span class="math notranslate nohighlight">\(i\)</span>. We can write this condition as:</p>
<div class="math notranslate nohighlight">
\[\forall i \in \left[m\right],\ y_i\left(\left\langle\mathbf{w, x_i}\right\rangle+b\right)&gt;0\]</div>
<p>All halfspaces <span class="math notranslate nohighlight">\(\left(\mathbf{w}, \mathcal{b}\right)\)</span> that satisfiy the condition above are <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> hypotheses (and their 0-1 error is zero). For any <strong>separable</strong> training set, there are many <span class="math notranslate nohighlight">\(\operatorname{ERM}\)</span> halfspaces, which one should we pick?</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;darkgrid&quot;</span><span class="p">)</span>

<span class="n">xp</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">yp</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">c</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">m1</span><span class="p">,</span> <span class="n">q1</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
<span class="n">m2</span><span class="p">,</span> <span class="n">q2</span> <span class="o">=</span> <span class="mf">1.3</span><span class="p">,</span> <span class="mi">0</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">m1</span> <span class="o">+</span> <span class="n">q1</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">m2</span> <span class="o">+</span> <span class="n">q2</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y1</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y2</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="s2">&quot;tab10&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;green&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">xp</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">yp</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">70</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">([])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">([])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/model_exploration_2_0.png" src="_images/model_exploration_2_0.png" />
</div>
</div>
<p>Intuitively we might choose the red hyperplane to split our data, in order to formalize this intuition we introduce the concept of margin.</p>
<div class="important admonition">
<p class="admonition-title">Margin</p>
<p>The margin of a hyperplane with respect to a training set is the minimal distance between a point in the training set and the hyperplane</p>
</div>
<p>Note that if a hyperplane has a large margin, even if we sligthly perturn each dataset instance, it will still separate the data <em>correctly</em>.</p>
<p>As we will see later, the larger is the margin that a halfspace has over the training sample, the smaller is the true error of the halfspace. For this reason the <em>Hard-SVM</em> rule is the rule that returns an <span class="math notranslate nohighlight">\(ERM\)</span> hyperplane that separates the training set with the largest possible margin.</p>
<ul class="simple">
<li><p>Claim: The distance between a point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the hyperplane defined by <span class="math notranslate nohighlight">\(\left(\mathbf{w},b\right)\)</span> where <span class="math notranslate nohighlight">\(\left\lVert\mathbb{w}\right\rVert=1\)</span> is <span class="math notranslate nohighlight">\(\left\lvert\left\langle\mathbf{w, x}\right\rangle+b\right\rvert\)</span></p></li>
</ul>
<p>On this basis, the point <span class="math notranslate nohighlight">\(\mathbf{x_i}\in S\)</span> closest to the hyperplane <span class="math notranslate nohighlight">\(L\)</span> is</p>
<div class="math notranslate nohighlight">
\[d\left(\mathbf{x},L\right)=\underset{i\in\left[m\right]}{\operatorname{min}}\left\lvert\left\langle\mathbf{w, x_i}\right\rangle+b\right\rvert\]</div>
<p>Hence, the Hard-SVM rule is</p>
<div class="important admonition">
<p class="admonition-title">Hard-SVM rule</p>
<div class="math notranslate nohighlight">
\[\underset{\left(\mathbf{w}, b\right):\left\lVert\mathbf{w}=1\right\rVert}{\operatorname{argmax}}\underset{i\in\left[m\right]}{\operatorname{min}}\left\lvert\left\langle\mathbf{w, x_i}\right\rangle+b\right\rvert\ \ s.t.\ \ \forall i, \ \mathcal{y_i}\left(\left\langle\mathbf{w, x_i}\right\rangle+b\right)&gt;0\tag{1}\]</div>
<p>Equivalently</p>
<div class="math notranslate nohighlight">
\[\underset{\left(\mathbf{w}, b\right):\left\lVert\mathbf{w}=1\right\rVert}{\operatorname{argmax}}\underset{i\in\left[m\right]}{\operatorname{min}}\mathcal{y_i}\left(\left\langle\mathbf{x, x_i}\right\rangle+b\right)\tag{2}\]</div>
<p>Equivalently</p>
<div class="math notranslate nohighlight">
\[\underset{\left(\mathbf{w}, b\right)}{\operatorname{argmin}}\left\lVert\mathbf{w}\right\rVert^2\ \operatorname{s.t.}\ \forall i,\ \mathcal{y_i}\left(\left\langle\mathbf{w, x_i}\right\rangle+b\right)&gt;1\tag{3}\]</div>
</div>
<p>Intuitively, we are going from maximizing the numerator of the distance formula (maximizing the minimal distance) to minimizing the norm of <span class="math notranslate nohighlight">\(w\)</span>.
The last equation can be solved via quadratic programming.</p>
<p>We now have to define a new concept of separability:</p>
<div class="important admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(\left(\gamma,\rho\right)\)</span>-margin</p>
<p>We say that <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is separable with a <span class="math notranslate nohighlight">\(\left(\gamma,\rho\right)\)</span>-margin if exists <span class="math notranslate nohighlight">\(\left(\mathbf{w^*}, b^*\right)\ \operatorname{s.t.}\ \left\lVert\mathbf{w^*}\right\rVert=1\)</span> and</p>
<div class="math notranslate nohighlight">
\[\mathcal{D}\left(\{\left(x, y\right): \left\lVert x\right\rVert\leq\rho \wedge \mathcal{y}\left(\left\langle\mathbf{w^*, x}\right\rangle+b^*\right)&gt;1 \}\right)=1\]</div>
<p>This means that when we say that <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is separable with a <span class="math notranslate nohighlight">\(\left(\gamma,\rho\right)\)</span>-margin, if there is a ball of radius <span class="math notranslate nohighlight">\(\rho\)</span> inside which the hyperplane can <strong>always</strong> separate the points (finite set of points can violate the separability)</p>
</div>
<p>We can now state the theorem that gives us the sample complexity for Hard-SVM (without proving it)</p>
<div class="important admonition">
<p class="admonition-title">Hard-SVM Sample complexity</p>
<p>if <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> is separable with a <span class="math notranslate nohighlight">\(\left(\gamma,\rho\right)\)</span>-margin, then the sample complexity of Hard-SVM is</p>
<div class="math notranslate nohighlight">
\[m\left(\epsilon,\delta\right)\leq \frac{8}{\epsilon^2}\left(2\left(\frac{\rho}{\gamma}\right)^2+\log{\frac{2}{\delta}}\right)\]</div>
<p>Note that the sample complexity obtained by computing the VC dimension of half spaces in <span class="math notranslate nohighlight">\(d\)</span> dimensions, we get that the sample complexity is <span class="math notranslate nohighlight">\(d\)</span>. While now we have found that the sample complexity does not depend on the dimension, but only on the <span class="math notranslate nohighlight">\(\frac{\rho}{\delta}\)</span> ratio (and the accuracy <span class="math notranslate nohighlight">\(\delta\)</span> and the correctness <span class="math notranslate nohighlight">\(\epsilon\)</span>)</p>
</div>
<div class="important admonition">
<p class="admonition-title">Robustness</p>
<p>One of the strong points of SVMs is that due to their theory implications, the solution of the svm rule can be written as a linear combination of the points on the margin, for this reason they are robust with respect to outliers. (Fritz-John lemma)</p>
</div>
</div>
<div class="section" id="soft-svm">
<h4>1.1.2 Soft-SVM<a class="headerlink" href="#soft-svm" title="Permalink to this headline">¶</a></h4>
<p>In the Hard-SVM settings we made the strong separability assumption, which needs to be relaxed in order to apply SVMs to real data. In order to relax this constraint, we introduce nonnegative slack variables <span class="math notranslate nohighlight">\(\xi_1,\dots,\xi_m\)</span>, and we repace each constraint that was before <span class="math notranslate nohighlight">\(\mathcal{y_i}\left(\left\langle\mathbf{w,x_i}\right\rangle+b\right) \geq 1\)</span> with <span class="math notranslate nohighlight">\(\mathcal{y_i}\left(\left\langle\mathbf{w,x_i}\right\rangle+b\right) \geq 1 - \xi_i\)</span>. <span class="math notranslate nohighlight">\(\xi_i\)</span> measures by how much the constraint <span class="math notranslate nohighlight">\(\mathcal{y_i}\left(\left\langle\mathbf{w,x_i}\right\rangle+b\right) \geq 1\)</span> is being violated. The Soft-SVM rules then writes:</p>
<div class="important admonition">
<p class="admonition-title">Soft-SVM rule</p>
<div class="math notranslate nohighlight">
\[\underset{\left(\mathbf{w}, b\right)}{\operatorname{argmin}}\left(\lambda\left\lVert\mathbf{w}\right\rVert^2+\frac{1}{m}\overset{m}{\underset{i=1}{\sum}}\xi_i\right) \operatorname{s.t.}\ \forall i,\ \mathcal{y_i}\left(\left\langle\mathbf{w, x_i}\right\rangle+b\right)\geq1\wedge\xi_i\geq0\tag{3}\]</div>
<p>Note that now we also minimize the average violation of the constraint along with the norm</p>
</div>
</div>
<div class="section" id="kernel-trick">
<h4>1.1.3 Kernel Trick<a class="headerlink" href="#kernel-trick" title="Permalink to this headline">¶</a></h4>
<p>Note that there exists a dual formulation for the SVM problem:</p>
<div class="important admonition">
<p class="admonition-title">Hard-SVM Primal-Dual</p>
<p>(Primal)</p>
<div class="math notranslate nohighlight">
\[\underset{\left(\mathbf{w}, b\right)}{\operatorname{argmin}}\left\lVert\mathbf{w}\right\rVert^2\ \operatorname{s.t.}\ \forall i,\ \mathcal{y_i}\left\langle\mathbf{w, x_i}\right\rangle\geq1\tag{3}\]</div>
<p>(Dual)</p>
<div class="math notranslate nohighlight">
\[\underset{\mathbf{\alpha}\in\mathbb{R}^m:\mathbf{\alpha}\geq\mathbf{0}}{\operatorname{max}}\underset{\mathbf{w}}{\operatorname{min}}\left(\frac{1}{2}\left\lVert\mathbf{w}^2\right\rVert+\overset{m}{\underset{i=1}{\sum}}\alpha_i\left(1-y_i\langle\mathbf{w, x_i}\rangle\right)\right)\]</div>
</div>
<p>We start by fixing <span class="math notranslate nohighlight">\(\alpha\)</span> and solving the inner optimization (we call it <span class="math notranslate nohighlight">\(Z\)</span>). Note that the inner minimization is the minimization of a hyperparaboloid in <span class="math notranslate nohighlight">\(w\)</span>, and in order to find the minimum, it is enough to find the vertex of the paraboloid, requiring the derivative equals 0</p>
<div class="math notranslate nohighlight">
\[\nabla Z\left(w\right)=\mathbf{w}-\overset{m}{\underset{i=1}{\sum}}\alpha_i y_i \mathbf{x}_i\]</div>
<div class="math notranslate nohighlight">
\[\mathbf{w}=\overset{m}{\underset{i=1}{\sum}}\alpha_i y_i \mathbf{x}_i\]</div>
<p>NOTE that this means that our solution is nothing more than a linear combination of our data. Plugging back the solution of the inner optimization, into the outer optimization, we obtain</p>
<div class="math notranslate nohighlight">
\[\underset{\mathbf{\alpha}\in\mathbb{R}^m:\mathbf{\alpha}\geq\mathbf{0}}{\operatorname{max}}\left(\overset{m}{\underset{i=1}{\sum}}\alpha_i-\frac{1}{2}\overset{m}{\underset{i,j=1}{\sum}} \alpha_i\alpha_j y_i y_j \left\langle\mathbf{x_i, x_j}\right\rangle  \right)\]</div>
<p>The problem now is entirely dependent on the inner product of <span class="math notranslate nohighlight">\(\left\langle\mathbf{x_i, x_j}\right\rangle\)</span>. We introudce the concept of <em>GRAM MATRIX</em> which is essentially a matrix of inner products defined as</p>
<div class="important admonition">
<p class="admonition-title">Gram Matrix</p>
<p>We call Gram Matrix of <span class="math notranslate nohighlight">\({\mathbf{x_i},\dots,\mathbf{x_m}}\)</span> the matrix <span class="math notranslate nohighlight">\(G=\left(G_{ij}\right)\ \operatorname{s.t.}\ G_{ij}=\left\langle\mathbf{x_i, x_j}\right\rangle\)</span></p>
</div>
<p>And we can now say that our problem is entirely controlled by the entries of the Gram Matrix. Now that we have this new formulation of the SVM, we can make a step further, and think to those situations where the points are not linearly separable in the original representation (space). The solution to this is to find some <em>feature space</em> where our data is actually linearly separable, and train our linear model in that space. In order to reach that space, we need a <em>mapping function</em></p>
<div class="math notranslate nohighlight">
\[\psi: \mathcal{X}\rightarrow\mathcal{H} \left(\text{e.g.}\psi\left(x\right)=\left(x,x^2\right)\right)\]</div>
<p>Where <span class="math notranslate nohighlight">\(\mathcal{H}\)</span> is a special space called <em>Hilbert Space</em>. Our solution turns into <span class="math notranslate nohighlight">\(\mathbf{w}=\underset{i}{\sum}\alpha_i\psi\left(\mathbf{x}_i\right)\)</span>, while the entry of our Gram matrix now looks like <span class="math notranslate nohighlight">\(G_{ij}=\left\langle\psi\left(\mathbf{x}_i\right),\psi\left(\mathbf{x}_j\right)\right\rangle\)</span></p>
<p>This is an important result, but it carries with it some computational issues (it is expensive to compute the full Gram matrix for any mapping function). To make this feasible we consider <em>Mercer’s conditions</em>:</p>
<div class="important admonition">
<p class="admonition-title">Mercer’s Conditions</p>
<p>A symmetric function (that is <span class="math notranslate nohighlight">\(K\left(\mathbf{x,x^\prime}\right)=K\left(\mathbf{x^\prime,x}\right)\ \forall x,x^\prime\)</span>) <span class="math notranslate nohighlight">\(K:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\)</span> implements an inner product in some Hilbert space, (that is <span class="math notranslate nohighlight">\(\exists\psi:\mathcal{X}\rightarrow\mathcal{H}:k\left(\mathbf{x,x^\prime}\right)=\left\langle\psi\left(\mathbf{x_i}\right),\psi\left(\mathbf{x_J}\right)\right\rangle\)</span> if and only if it is positive semidefinite.</p>
</div>
<p>This allows to replace the dot product in the feature space, with the evaluation of <span class="math notranslate nohighlight">\(K\)</span> on the points in the starting representation, making this approach viable computationally.</p>
</div>
</div>
<div class="section" id="application">
<h3>1.2 Application<a class="headerlink" href="#application" title="Permalink to this headline">¶</a></h3>
<p>We now use the code we showed in the previous section and perform grid-search cross-validation using the Sklearn’s implementation of the Soft-SVMs.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./dataset/online_shoppers_intention.csv&#39;</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;Revenue&#39;</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;Revenue&#39;</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>

<span class="n">textual_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;VisitorType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weekend&#39;</span><span class="p">]</span>
<span class="n">categorical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;OperatingSystems&#39;</span><span class="p">,</span> <span class="s1">&#39;Browser&#39;</span><span class="p">,</span> <span class="s1">&#39;Region&#39;</span><span class="p">,</span> <span class="s1">&#39;TrafficType&#39;</span><span class="p">,</span> <span class="s1">&#39;VisitorType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weekend&#39;</span><span class="p">]</span>
<span class="n">numerical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Administrative&#39;</span><span class="p">,</span> <span class="s1">&#39;Administrative_Duration&#39;</span><span class="p">,</span> <span class="s1">&#39;Informational&#39;</span><span class="p">,</span> <span class="s1">&#39;Informational_Duration&#39;</span><span class="p">,</span> <span class="s1">&#39;ProductRelated&#39;</span><span class="p">,</span> <span class="s1">&#39;ProductRelated_Duration&#39;</span><span class="p">,</span> <span class="s1">&#39;BounceRates&#39;</span><span class="p">,</span> <span class="s1">&#39;ExitRates&#39;</span><span class="p">,</span> <span class="s1">&#39;PageValues&#39;</span><span class="p">]</span>

<span class="n">column_transformer</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;OrdinalEncoder&#39;</span><span class="p">,</span> <span class="n">OrdinalEncoder</span><span class="p">(),</span> <span class="n">textual_columns</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;MinMaxScaler&#39;</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">(),</span> <span class="n">numerical_columns</span><span class="p">),</span>
<span class="c1">#         (&#39;OneHotEncoder&#39;, OneHotEncoder(), categorical_columns),</span>
    <span class="p">],</span>
    <span class="n">remainder</span><span class="o">=</span><span class="s1">&#39;passthrough&#39;</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTENC</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">categorical_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;OperatingSystems&#39;</span><span class="p">,</span> <span class="s1">&#39;Browser&#39;</span><span class="p">,</span> <span class="s1">&#39;Region&#39;</span><span class="p">,</span> <span class="s1">&#39;TrafficType&#39;</span><span class="p">,</span> <span class="s1">&#39;VisitorType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weekend&#39;</span><span class="p">]</span>
<span class="n">categorical_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="ow">in</span> <span class="n">categorical_features</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_train</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;ColumnTransformer&#39;</span><span class="p">,</span> <span class="n">column_transformer</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;SMOTENC&#39;</span><span class="p">,</span> <span class="n">SMOTENC</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical_indices</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Classifier&#39;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">())</span>
    <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Below there is a list of the parameters accepted by Sklearn’s Support Vector Classifiers</p>
<ul class="simple">
<li><p><strong>C</strong>: A value inversely proportional the amount of slack we want to allow.</p></li>
<li><p><strong>kernel</strong>: The kernel function used to <em>compute</em> the inner products in the feature space.</p></li>
<li><p><strong>gamma</strong>: A coefficient multiplying the kernel function. Intuitively is the inverse of the radius of influence of samples seleced as support vectors.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Here we define the subset of parameters to use in the gridsearch model selection technique</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">&#39;Classifier__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__gamma&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># linear_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=6, verbose=5).fit(x_train, y_train)</span>
<span class="c1"># linear_search.cv_results_</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="trees-and-random-forests">
<h2>2. Trees and Random Forests<a class="headerlink" href="#trees-and-random-forests" title="Permalink to this headline">¶</a></h2>
<p>Decision trees are predictors that assign labels to samples by travelling across a tree, from root to leaf. On each node, the next child is chosen by splitting the input space.</p>
<p>The most common splitting rule for an internal node of the tree is a thresold on the value of a single feature (e.g. temperature&gt;40). This means that we move to either one of the child on the basis of <span class="math notranslate nohighlight">\(\mathbb{1}_\left[x_i&lt;\theta\right]\)</span> where <span class="math notranslate nohighlight">\(i \in \left[d\right]\)</span> is the index of the feature currently being analyzed, and <span class="math notranslate nohighlight">\(\theta\in\mathbb{R}\)</span> is the threshold value. We can think of a decision tree as a splitting of the input space <span class="math notranslate nohighlight">\(\mathcal{X}\in\mathbb{R}^d\)</span>, into as many cells as there are leaf nodes. It is clear then that the tree with <span class="math notranslate nohighlight">\(k\)</span> leaves shatters a set of cardinality <span class="math notranslate nohighlight">\(k\)</span>. It follows that an infinitely tall tree, with infinite number of leafs, has an infinite VC dimension. This can cause overfitting. For this reason we must adopt some methods to learn smaller trees, that will tend to overfit less, but have a higher empirical risk.</p>
<div class="section" id="growing-a-tree">
<h3>2.1. Growing a Tree<a class="headerlink" href="#growing-a-tree" title="Permalink to this headline">¶</a></h3>
<p>When building a tree, we start from the whole training set. Then we build recursively either a child or a leaf. Leafs are built when the labels of the remaining training set have all the same label (we are in a binary classification example) or when we have no samples left, while children are built when the remaining samples have mixed labels. When a child node is required, the threhold rule is built by considering some <em>Gain measure</em>, where we select the threshold on the feature that produces the maximum gain over the remaining samples. We then generate the two new children, and split our training set on the newly found feature with the newly found threshold. We go on until we encounter a leaf, and then backtrack.</p>
<p>This technique tends to build very big trees, for this reason some pruning techniques must be applied to limit the overfitting.</p>
</div>
<div class="section" id="from-trees-to-forests">
<h3>2.2. From Trees to Forests<a class="headerlink" href="#from-trees-to-forests" title="Permalink to this headline">¶</a></h3>
<p>A Random Forest is an <em>ensamble</em> of trees. Each tree is built by applying the same algorithm <span class="math notranslate nohighlight">\(A\)</span> over a training set <span class="math notranslate nohighlight">\(S^\prime\)</span> obtained by uniform sampling over <span class="math notranslate nohighlight">\(S\)</span> with replacement (<em>bagging</em>). Since if we only applied bagging to the samples, we would end up with very similarly structured tree (the first split would always be the same with respect to some metric), we also need to apply <em>feature bagging</em>. So we sample the <em>features</em> to be used, by choosing <span class="math notranslate nohighlight">\(k\)</span> features uniformly among the starting <span class="math notranslate nohighlight">\(d\)</span> features.</p>
</div>
<div class="section" id="bagging-and-cv">
<h3>2.3 Bagging and CV<a class="headerlink" href="#bagging-and-cv" title="Permalink to this headline">¶</a></h3>
<p>When applying the random forest algorithm with cross validation (and grid seach) the whole process becomes quite hard to picture, for this reason, below there is a visual recap of the whole process</p>
<p><img alt="trees_cv" src="_images/trees-cv.png" /></p>
</div>
<div class="section" id="id1">
<h3>2.4 Application<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;./dataset/online_shoppers_intention.csv&#39;</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;Revenue&#39;</span><span class="p">),</span> <span class="n">df_train</span><span class="p">[</span><span class="s1">&#39;Revenue&#39;</span><span class="p">]</span>

<span class="kn">from</span> <span class="nn">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">,</span> <span class="n">OneHotEncoder</span>

<span class="n">textual_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;VisitorType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weekend&#39;</span><span class="p">]</span>
<span class="n">categorical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;OperatingSystems&#39;</span><span class="p">,</span> <span class="s1">&#39;Browser&#39;</span><span class="p">,</span> <span class="s1">&#39;Region&#39;</span><span class="p">,</span> <span class="s1">&#39;TrafficType&#39;</span><span class="p">,</span> <span class="s1">&#39;VisitorType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weekend&#39;</span><span class="p">]</span>
<span class="n">numerical_columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Administrative&#39;</span><span class="p">,</span> <span class="s1">&#39;Administrative_Duration&#39;</span><span class="p">,</span> <span class="s1">&#39;Informational&#39;</span><span class="p">,</span> <span class="s1">&#39;Informational_Duration&#39;</span><span class="p">,</span> <span class="s1">&#39;ProductRelated&#39;</span><span class="p">,</span> <span class="s1">&#39;ProductRelated_Duration&#39;</span><span class="p">,</span> <span class="s1">&#39;BounceRates&#39;</span><span class="p">,</span> <span class="s1">&#39;ExitRates&#39;</span><span class="p">,</span> <span class="s1">&#39;PageValues&#39;</span><span class="p">]</span>

<span class="n">column_transformer</span> <span class="o">=</span> <span class="n">ColumnTransformer</span><span class="p">([</span>
        <span class="p">(</span><span class="s1">&#39;OrdinalEncoder&#39;</span><span class="p">,</span> <span class="n">OrdinalEncoder</span><span class="p">(),</span> <span class="n">textual_columns</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;MinMaxScaler&#39;</span><span class="p">,</span> <span class="n">MinMaxScaler</span><span class="p">(),</span> <span class="n">numerical_columns</span><span class="p">),</span>
<span class="c1">#         (&#39;OneHotEncoder&#39;, OneHotEncoder(), categorical_columns),</span>
    <span class="p">],</span>
    <span class="n">remainder</span><span class="o">=</span><span class="s1">&#39;passthrough&#39;</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="nn">imblearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTENC</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="n">categorical_features</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;OperatingSystems&#39;</span><span class="p">,</span> <span class="s1">&#39;Browser&#39;</span><span class="p">,</span> <span class="s1">&#39;Region&#39;</span><span class="p">,</span> <span class="s1">&#39;TrafficType&#39;</span><span class="p">,</span> <span class="s1">&#39;VisitorType&#39;</span><span class="p">,</span> <span class="s1">&#39;Weekend&#39;</span><span class="p">]</span>
<span class="n">categorical_indices</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="ow">in</span> <span class="n">categorical_features</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_train</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">(</span>
    <span class="n">steps</span><span class="o">=</span><span class="p">[</span>
        <span class="p">(</span><span class="s1">&#39;ColumnTransformer&#39;</span><span class="p">,</span> <span class="n">column_transformer</span><span class="p">),</span>
        <span class="p">(</span><span class="s1">&#39;SMOTENC&#39;</span><span class="p">,</span> <span class="n">SMOTENC</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="n">categorical_indices</span><span class="p">)),</span>
        <span class="p">(</span><span class="s1">&#39;Classifier&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">())</span>
    <span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Below there is a list of the parameters accepted by Sklearn’s Support Vector Classifiers
​</p>
<ul class="simple">
<li><p><strong>n_estimators</strong>: The number of trees to grow inside the forest.</p></li>
<li><p><strong>criterion</strong>: The criterion used to perform the split in the nodes.</p></li>
<li><p><strong>max_depth</strong>: Limits the size of the tree (lower values reduce overfitting)</p></li>
<li><p><strong>min_samples_split</strong>: The mimimum amount of samples in a node to allow a further split (high values reduce overfitting)</p></li>
<li><p><strong>min_samples_leaf</strong>: The minimum number of samples in both branches for a given node (high values reduce overfitting)</p></li>
<li><p><strong>max_features</strong>: The maximum number of features to use when splitting the nodes (maximum size for feature bagging)</p></li>
<li><p><strong>bootrtrap</strong>: Whether to use bootstrap when buildig trees, or use the entire dataset each time.</p></li>
<li><p><strong>oob_score</strong>: Whether to use out-of-bag-samples to estimate the generalization score. (not sure how it works when applying cross validation)
​</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Here we define the subset of parameters to use in the gridsearch model selection technique</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span>
        <span class="s1">&#39;Classifier__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__criterion&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">,</span> <span class="s1">&#39;entropy&#39;</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="s1">&#39;sqrt&#39;</span><span class="p">,</span> <span class="s1">&#39;log2&#39;</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__oob_score&#39;</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">],</span>
        <span class="s1">&#39;Classifier__random_state&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">42</span><span class="p">]</span>
    <span class="p">}</span>
<span class="p">]</span>

<span class="c1"># Since I have to run every notebook each time I want to update the whole jupyter-book, and I have very limited computational</span>
<span class="c1"># resources, I will comment the actual training and report only the best configuration found.</span>
<span class="c1"># Since I fixed the random state, it should be very easy to reproduce the results.</span>

<span class="c1"># linear_search = GridSearchCV(clf, param_grid, cv=5, n_jobs=6, verbose=5).fit(x_train, y_train)</span>
<span class="c1"># linear_search.cv_results_</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pprint</span> <span class="kn">import</span> <span class="n">pprint</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;best model &amp; score&#39;</span><span class="p">)</span>
<span class="c1"># pprint(linear_search.best_params_, compact=True)</span>
<span class="c1"># print(linear_search.best_score_)</span>
<span class="n">pprint</span><span class="p">({</span><span class="s1">&#39;Classifier__criterion&#39;</span><span class="p">:</span> <span class="s1">&#39;gini&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Classifier__max_depth&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
 <span class="s1">&#39;Classifier__max_features&#39;</span><span class="p">:</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Classifier__n_estimators&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
 <span class="s1">&#39;Classifier__oob_score&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
 <span class="s1">&#39;Classifier__random_state&#39;</span><span class="p">:</span> <span class="mi">42</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="mf">0.8687141522110355</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>best model &amp; score
{&#39;Classifier__criterion&#39;: &#39;gini&#39;,
 &#39;Classifier__max_depth&#39;: 50,
 &#39;Classifier__max_features&#39;: &#39;auto&#39;,
 &#39;Classifier__n_estimators&#39;: 50,
 &#39;Classifier__oob_score&#39;: True,
 &#39;Classifier__random_state&#39;: 42}
0.8687141522110355
</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="data_preparation.html" title="previous page">Pipelines</a>
    <a class='right-next' id="next-link" href="appendix.html" title="next page">Appendix</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Nicola Occelli<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>